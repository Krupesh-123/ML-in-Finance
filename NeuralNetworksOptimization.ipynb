{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Part IIB**\n",
        "\n",
        "MLF_GP1_CreditScore.csv file has credit score data in it. The dataset comprises information on 1700 firms from various industries, including 26 financial and accounting metrics. The last column indicates the rating assigned by Moody's, while the second-to-last column specifies whether the assets are of investment grade or not. Rating of Aaa, Aa1, Aa2, Aa3, A1, A2, A3, Baa1, Baa2, or Baa3 is considered as investment grade. \n",
        "\n",
        "**Objective**\n",
        "\n",
        "The objective is to categorize a company's credit rating into 16 categories and determine if the company is an investment grade. To achieve this, we divide the dataset into training and testing sets in an 80:20 ratio."
      ],
      "metadata": {
        "id": "dnNenZh1P_Ry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Linear regression**\n",
        "\n",
        "Linear regression is a statistical approach for determining the connection between one or more independent variables and a dependent variable. The purpose of linear regression is to find the optimal line that describes the connection between the dependent and independent variables.\n",
        "\n",
        "Ridge and Lasso are linear regression regularisation techniques that add a penalty term to the cost function to prevent overfitting. Ridge regression incorporates an L2 penalty term into the cost function, whereas Lasso regression incorporates an L1 penalty term. The penalty term reduces the coefficients to zero, resulting in a simpler and more understandable model.\n",
        "\n",
        "The **L2 penalty term** in **Ridge** regression is proportional to the square of the magnitude of the coefficients. This results in lower and more uniformly distributed coefficients, which reduces the effect of multicollinearity (the presence of two or more highly correlated independent variables).\n",
        "\n",
        "The **L1 penalty term** in **Lasso** regression is proportional to the absolute value of the coefficients. This results in a sparse model with many coefficients that are absolutely zero. Because it may automatically reject irrelevant or redundant independent variables, Lasso regression is beneficial for feature selection.\n"
      ],
      "metadata": {
        "id": "Jwyxh5I3UD8P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CODE :**\n",
        "\n",
        "The below programme uses linear regression with **Ridge and Lasso regularisation** to classify credit scores. It begins by loading and separating the \"**MLF_GP1_CreditScore.csv**\" file into training and test sets. For a Ridge regression model, it then turns the target variable into binary labels using **LabelEncoder** and standardises the input features using **StandardScaler**. The model is trained on test data with **alpha = 0.985** and predictions are made. The predicted target probabilities are then turned into binary labels with a threshold of **0.5**, and the label encoder is used to convert them back into their original categories values.\n",
        "\n",
        "The model's performance is evaluated using **confusion matrix and accuracy score**. Then, the code fits a Lasso regression model on the standardized input features and target values with **alpha=0.1 and max_iter=5000**, and predictions are made on the test data. Finally, the predicted values are converted into binary labels using a **threshold of 0.5** and transformed back into their corresponding investment grade labels using the label encoder. The performance of the Lasso model is evaluated using **confusion matrix and accuracy score**."
      ],
      "metadata": {
        "id": "PQtKbymYU-DL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# loading the CreditScore creditScoreData file\n",
        "creditScoreData = pd.read_csv(\"MLF_GP1_CreditScore.csv\")\n",
        "\n",
        "# Dividing whole data into training and test sets\n",
        "inputFeatures_trainSet_ridge, inputFeatures_testSet_ridge, target_trainSet_ridge, target_testSet_ridge = train_test_split(creditScoreData.iloc[:, :-2], creditScoreData[\"InvGrd\"], test_size=0.2, random_state=50)\n",
        "\n",
        "# convert the target variable into binary labels using the LabelEncoder function.\n",
        "lblEncoder = LabelEncoder()\n",
        "traget_train_ridge_binary = lblEncoder.fit_transform(target_trainSet_ridge)\n",
        "traget_test_ridge_binary = lblEncoder.transform(target_testSet_ridge)\n",
        "\n",
        "# standardizes the input features using the StandardScaler function for a ridge regression model\n",
        "stdScaler = StandardScaler()\n",
        "inputFeatures_trainSet_scaled = stdScaler.fit_transform(inputFeatures_trainSet_ridge)\n",
        "inputFeatures_testSet_ridge_scaled = stdScaler.transform(inputFeatures_testSet_ridge)\n",
        "\n",
        "# create ridge regression model with regularization parameter alpha = 0.985 and trains the model \n",
        "modelRidgeReg = Ridge(alpha=0.985)\n",
        "modelRidgeReg.fit(inputFeatures_trainSet_scaled, traget_train_ridge_binary)\n",
        "\n",
        "# predicts the target labels for the test data using the trained ridge regression model\n",
        "targetPredictedRidge = modelRidgeReg.predict(inputFeatures_testSet_ridge_scaled)\n",
        "\n",
        "# converts the predicted target probabilities into binary labels using a threshold of 0.5\n",
        "targetPredictedBinary = (targetPredictedRidge >= 0.5).astype(int)\n",
        "\n",
        "# converts the predicted binary labels back into their original categorical values using the label encoder\n",
        "targetPredictedInvGrade = lblEncoder.inverse_transform(targetPredictedBinary)\n",
        "targetTestInvGradeRidge = lblEncoder.inverse_transform(traget_test_ridge_binary)\n",
        "\n",
        "# Evaluate model performance\n",
        "conf_matrix = confusion_matrix(traget_test_ridge_binary, targetPredictedBinary)\n",
        "accScore = accuracy_score(traget_test_ridge_binary, targetPredictedBinary)\n",
        "\n",
        "# Displaying the Ridge Regression evaluaiton metrics\n",
        "print(\"#Ridge Regression evaluaiton metrics#\")\n",
        "print(\"Confusion matrix of Ridge Regression model:\")\n",
        "print(conf_matrix)\n",
        "print(\"Ridge Regression Model Predicted investment grade:\")\n",
        "print(targetPredictedInvGrade)\n",
        "print(\"Ridge Regression Model Actual investment grade\")\n",
        "print(targetTestInvGradeRidge)\n",
        "print(f\"Accuracy of Ridge Regression model: {accScore}\")\n",
        "\n",
        "# Fit Lasso regression model on the standardized input features and target values.\n",
        "regressionLassoModel = Lasso(alpha=0.1, max_iter=5000)\n",
        "regressionLassoModel.fit(inputFeatures_trainSet_scaled, traget_train_ridge_binary)\n",
        "\n",
        "# predict the target variable for the test data using the trained Lasso regression model\n",
        "targetPredictedLasso = regressionLassoModel.predict(inputFeatures_testSet_ridge_scaled)\n",
        "\n",
        "# predicts the target variable as binary by converting the predicted values >= 0.5 to 1, otherwise 0\n",
        "targetPredictedBinary = (targetPredictedLasso >= 0.5).astype(int)\n",
        "\n",
        "#  Transforms predicted binary values to their corresponding investment grade labels and test target to investment grade labels\n",
        "targetPredictedInvGrade = lblEncoder.inverse_transform(targetPredictedBinary)\n",
        "targetTestInvGradeRidge = lblEncoder.inverse_transform(traget_test_ridge_binary)\n",
        "\n",
        "# Evaluate the performance of the model\n",
        "accuracy_Score = accuracy_score(traget_test_ridge_binary, targetPredictedBinary)\n",
        "confus_matrix = confusion_matrix(traget_test_ridge_binary, targetPredictedBinary)\n",
        "\n",
        "# Print the results for Lasso Regression evaluaiton metrics\n",
        "print(\"#Lasso Regression Results#\")\n",
        "print(\"Lasso model Predicted investment grade labels:\")\n",
        "print(targetPredictedInvGrade)\n",
        "print(\"Lasso model Actual investment grade labels:\")\n",
        "print(targetTestInvGradeRidge)\n",
        "print(f\"Accuracy of Lasso model: {accuracy_Score}\")\n",
        "print(\"Confusion matrix of Lasso model:\")\n",
        "print(confus_matrix)\n"
      ],
      "metadata": {
        "id": "V5Vxpi-GfCF9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a119287a-821e-430d-88c0-0bd1ca7c6156"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#Ridge Regression evaluaiton metrics#\n",
            "Confusion matrix of Ridge Regression model:\n",
            "[[  4  62]\n",
            " [  1 273]]\n",
            "Ridge Regression Model Predicted investment grade:\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1]\n",
            "Ridge Regression Model Actual investment grade\n",
            "[0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1\n",
            " 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 0 1 1 1]\n",
            "Accuracy of Ridge Regression model: 0.8147058823529412\n",
            "#Lasso Regression Results#\n",
            "Lasso model Predicted investment grade labels:\n",
            "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1]\n",
            "Lasso model Actual investment grade labels:\n",
            "[0 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 0\n",
            " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 1\n",
            " 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 0 0 1 0 1 1 1 1 0 1\n",
            " 1 1 0 1 0 0 1 1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1\n",
            " 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 1\n",
            " 0 1 1 1 0 1 1 1 0 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 0 1 0 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 1 0 0 0 1 1 1 1 1 0 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1\n",
            " 1 0 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1 0 0 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1\n",
            " 1 1 1 0 1 1 1]\n",
            "Accuracy of Lasso model: 0.8058823529411765\n",
            "Confusion matrix of Lasso model:\n",
            "[[  0  66]\n",
            " [  0 274]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ridge Regression evaluaiton metrics**\n",
        "\n",
        "Confusion matrix of Ridge Regression model:\n",
        "\n",
        "    [[  4  62 ]\n",
        "\n",
        "     [  1 273 ]]\n",
        "\n",
        "Accuracy of Ridge Regression model: 0.8147058823529412\n",
        "\n",
        "\n",
        "**Lasso Regression evaluaiton metrics**\n",
        "\n",
        "Confusion matrix of Lasso model:\n",
        "\n",
        "    [[  0  66]\n",
        "     [  0 274] ]\n",
        "\n",
        "Accuracy of Lasso Regression model: 0.8058823529411765\n"
      ],
      "metadata": {
        "id": "-IudPWBSWWyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic regression**:\n",
        "\n",
        "Logistic regression is a machine learning approach that is commonly used to solve binary classification problems. Based on the values of the input attributes, it estimates the likelihood of an instance belonging to a specific class (such as 0 or 1). **Ridge and Lasso** regularisation are logistic regression techniques intended to prevent overfitting, which occurs when the model is overly complex and performs well on training data but badly on testing data.\n",
        "\n",
        "**Ridge regularisation** introduces a penalty term into the loss function that is proportional to the square of the coefficient magnitude. This penalty term reduces the coefficient values to zero, lowering the impact of irrelevant characteristics and improving generalisation performance.\n",
        "\n",
        "The penalty term in **Lasso regularisation** is proportional to the absolute value of the coefficients. This penalty term effectively performs feature selection and improves the model's interpretability by forcing some of the coefficient values to become exactly zero.\n",
        "\n",
        "To summarise, adopting Ridge or Lasso regularisation in logistic regression helps to reduce **overfitting**, improves the model's generalisation performance, and can improve its interpretability.\n",
        "\n",
        "**CODE :**\n",
        "\n",
        "On a credit score dataset, this code does binary classification using **logistic regression** models with **L1 and L2 regularisation**. Using the pandas package, the dataset is loaded from a CSV file. The categorical variable **'Rating'** is one-hot encoded using the scikit-learn library's **OneHotEncoder** function. The encoded category variable is concatenated with the original data to create a new pandas DataFrame. The data is then divided into training and testing sets in an 80/20 ratio using the train_test_split function from the scikit-learn library.\n",
        "\n",
        "The **LogisticRegression function from the scikit-learn library** is used to generate two logistic regression models with L1 and L2 regularisation, respectively. The training set is used to train both models. The test data is used to extract the target variables for investment grade prediction.\n",
        "\n",
        "The trained models are used to forecast the test data's target variables. Both models' evaluation metrics (**accuracy, precision, recall, F1 score, and AUC-ROC**) are obtained using scikit-learn metrics routines. Finally, the evaluation metrics for both models are produced.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9YMZmUfpXnKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# loading the CreditScore creditScoreData file\n",
        "creditScoreData = pd.read_csv(\"MLF_GP1_CreditScore.csv\")\n",
        "\n",
        "\n",
        "# one-hot encodes the categorical variable \"Rating\" using the OneHotEncoder function\n",
        "ohEnc = OneHotEncoder()\n",
        "encRatings = ohEnc.fit_transform(creditScoreData['Rating'].values.reshape(-1,1)).toarray()\n",
        "\n",
        "# concatenate the creditScoreData and the one-hot encoded \"Rating\" variable into a new pandas DataFrame\n",
        "creditScoreData_encoded = pd.concat([creditScoreData, pd.DataFrame(encRatings)], axis=1)\n",
        "# Print the first 5 rows of creditScoreData_encoded dataset\n",
        "print(\"First five rows of dataset:\")\n",
        "print(creditScoreData_encoded.head())\n",
        "\n",
        "#  split the dataset creditScoreData_encoded into train and test sets with a 80/20 ratio\n",
        "train_creditScoreData_encoded, test_creditScoreData_encoded = train_test_split(creditScoreData_encoded, test_size=0.2, random_state=50)\n",
        "\n",
        "# splits the train and test data into input features and target variables for investment grade prediction\n",
        "inputFeatures_test = test_creditScoreData_encoded.iloc[:, :-18]\n",
        "target_test_invstmntGrade = test_creditScoreData_encoded.iloc[:, -2]\n",
        "\n",
        "inputFeatures_train = train_creditScoreData_encoded.iloc[:, :-18]\n",
        "target_train_invstmntGrade = train_creditScoreData_encoded.iloc[:, -2]\n",
        "\n",
        "\n",
        "#Model 2 Logistic regression:\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# create a logistic regression model with L1 regularization using the solver 'liblinear' and trains the model on the training data\n",
        "lrmodel_ridge = LogisticRegression(penalty='l1', solver='liblinear', random_state=50)\n",
        "lrmodel_ridge.fit(inputFeatures_train, target_train_invstmntGrade)\n",
        "\n",
        "# predicts target labels for the test data using the trained logistic regression model\n",
        "predictedTargetInvstmntGrade = lrmodel_ridge.predict(inputFeatures_test)\n",
        "\n",
        "# compute the evaluation metrics (accuracy, precision, recall, F1 score, and AUC-ROC) for the ridge logistic regression model\n",
        "prc_lrmodel_ridge = precision_score(target_test_invstmntGrade, predictedTargetInvstmntGrade)\n",
        "accur_lrmodel_ridge = accuracy_score(target_test_invstmntGrade, predictedTargetInvstmntGrade)\n",
        "recl_lrmodel_ridge = recall_score(target_test_invstmntGrade, predictedTargetInvstmntGrade)\n",
        "aucRoc_lrmodel_ridge = roc_auc_score(target_test_invstmntGrade, predictedTargetInvstmntGrade)\n",
        "f1_lrmodel_ridge = f1_score(target_test_invstmntGrade, predictedTargetInvstmntGrade)\n",
        "\n",
        "\n",
        "# print the evaluation metrics\n",
        "print(\"Ridge Logistic Regression:\")\n",
        "print(\"Precision:\", prc_lrmodel_ridge)\n",
        "print(\"Accuracy:\", accur_lrmodel_ridge)\n",
        "print(\"Recall:\", recl_lrmodel_ridge)\n",
        "print(\"AUC-ROC:\", aucRoc_lrmodel_ridge)\n",
        "print(\"F1-score:\", f1_lrmodel_ridge)\n",
        "\n",
        "\n",
        "# create a logistic regression model with L2 regularization using the solver 'liblinear' and trains the model on the training data\n",
        "lrmodel_lasso = LogisticRegression(penalty='l2', solver='liblinear', random_state=50)\n",
        "lrmodel_lasso.fit(inputFeatures_train, target_train_invstmntGrade)\n",
        "\n",
        "# make predictions on the test creditScoreData\n",
        "predictedTargetInvstmntGradeLasso = lrmodel_lasso.predict(inputFeatures_test)\n",
        "\n",
        "# compute the evaluation metrics (accuracy, precision, recall, F1 score, and AUC-ROC) for the lasso logistic regression model\n",
        "prc_lrmodel_lasso = precision_score(target_test_invstmntGrade, predictedTargetInvstmntGradeLasso)\n",
        "accur_lrmodel_lasso = accuracy_score(target_test_invstmntGrade, predictedTargetInvstmntGradeLasso)\n",
        "aucRoc_lrmodel_lasso = roc_auc_score(target_test_invstmntGrade, predictedTargetInvstmntGradeLasso)\n",
        "recl_lrmodel_lasso = recall_score(target_test_invstmntGrade, predictedTargetInvstmntGradeLasso)\n",
        "f1_lrmodel_lasso = f1_score(target_test_invstmntGrade, predictedTargetInvstmntGradeLasso)\n",
        "\n",
        "\n",
        "# print the evaluation metrics\n",
        "print(\"Lasso Logistic Regression:\")\n",
        "print(\"Precision:\", prc_lrmodel_lasso)\n",
        "print(\"Accuracy:\", accur_lrmodel_lasso)\n",
        "print(\"AUC-ROC:\", aucRoc_lrmodel_lasso)\n",
        "print(\"Recall:\", recl_lrmodel_lasso)\n",
        "print(\"F1-score:\", f1_lrmodel_lasso)\n"
      ],
      "metadata": {
        "id": "UwFvU2Pzd6Dp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d955ceee-5f71-48c1-f93d-07009a44d626"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First five rows of dataset:\n",
            "   Sales/Revenues  Gross Margin    EBITDA  EBITDA Margin  \\\n",
            "0       -0.005496      0.030763  0.018885       0.024515   \n",
            "1       -0.005496      0.030763  0.088716       0.094733   \n",
            "2       -0.007045      0.023159  0.088716       0.096440   \n",
            "3       -0.009396      0.028400  0.088716       0.099046   \n",
            "4       -0.009009      0.027714  0.088716       0.098611   \n",
            "\n",
            "   Net Income Before Extras  Total Debt  Net Debt   LT Debt   ST Debt  \\\n",
            "0                  0.146849   -0.029710 -0.019296 -0.042648  0.049875   \n",
            "1                  0.146849   -0.029710 -0.019296 -0.042648  0.049875   \n",
            "2                  0.108590    0.039410  0.034268  0.009059  0.250371   \n",
            "3                  0.146137    0.030071  0.036938 -0.016964  0.356994   \n",
            "4                  0.123500    0.024224  0.034445 -0.034132  0.461894   \n",
            "\n",
            "       Cash  ...    6    7    8    9   10   11   12   13   14   15  \n",
            "0 -0.133716  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "1 -0.133716  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "2  0.101315  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "3 -0.052606  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "4 -0.090869  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
            "\n",
            "[5 rows x 44 columns]\n",
            "Ridge Logistic Regression:\n",
            "Precision: 0.2\n",
            "Accuracy: 0.8382352941176471\n",
            "Recall: 0.019230769230769232\n",
            "AUC-ROC: 0.5026709401709403\n",
            "F1-score: 0.03508771929824562\n",
            "Lasso Logistic Regression:\n",
            "Precision: 0.2\n",
            "Accuracy: 0.8382352941176471\n",
            "AUC-ROC: 0.5026709401709403\n",
            "Recall: 0.019230769230769232\n",
            "F1-score: 0.03508771929824562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ridge Logistic Regression Evaluation Metrics**:\n",
        "\n",
        "Precision: 0.2\n",
        "\n",
        "Accuracy: 0.8382352941176471\n",
        "\n",
        "Recall: 0.019230769230769232\n",
        "\n",
        "AUC-ROC: 0.5026709401709403\n",
        "\n",
        "F1-score: 0.03508771929824562\n",
        "\n",
        "The assessment metrics displayed are for the Ridge Logistic Regression model's performance on a binary classification test. Each metric is defined as follows:\n",
        "\n",
        "- Precision is defined as the ratio of true positive predictions to total positive predictions. In this scenario, it means that just 20% of all anticipated positive instances were real positive. \n",
        "\n",
        "- Accuracy: This is the ratio of correct forecasts to total predictions made. In this situation, the model's accuracy is 83.82%, which means that 83.82% of all predictions were true.\n",
        "\n",
        "- Recall: This is the ratio of true positive predictions to total positive cases in the data. In this scenario, it means that the model correctly identified only 1.92% of all positive instances.\n",
        "\n",
        "- AUC-ROC. The ROC curve is a graph that compares the true positive rate to the false positive rate at various categorization levels. The AUC-ROC score ranges from 0 to 1, with 0.5 representing random guessing and 1 representing perfect classification. The AUC-ROC score in this situation is 0.502, indicating that the model performs only marginally better than random guessing.\n",
        "\n",
        "- F1-score: This is the harmonic mean of precision and recall. It achieves a good combination of precision and recall. The F1-score in this situation is 0.035, indicating that the model is underperforming in terms of both precision and recall.\n",
        "\n",
        "\n",
        "**Lasso Logistic Regression Evaluation Metrics:**\n",
        "\n",
        "Precision: 0.2\n",
        "\n",
        "Accuracy: 0.8382352941176471\n",
        "\n",
        "AUC-ROC: 0.5026709401709403\n",
        "\n",
        "Recall: 0.019230769230769232\n",
        "\n",
        "F1-score: 0.03508771929824562\n",
        "\n",
        "These are the measures for evaluating a Lasso logistic regression model.\n",
        "\n",
        "Precision is defined as the proportion of genuine positive predictions to total anticipated positives. A accuracy of 0.2 suggests that just 20% of all anticipated positive instances were true positives.\n",
        "\n",
        "Accuracy is defined as the proportion of correctly classified cases to total instances. The model accurately classified 83.82% of the occurrences with an accuracy of 0.8382.\n",
        "\n",
        "AUC-ROC stands for Area Under the Receiver Operating Characteristic Curve, and it indicates the model's ability to differentiate between positive and negative classes. An AUC-ROC of 0.5027 indicates that the model's ability to differentiate across classes is comparable to random chance.\n",
        "\n",
        "Recall is defined as the proportion of genuine positive predictions to total actual positives. A recall of 0.0192 indicates that the model accurately detected just 1.92% of all actual positive events.\n",
        "\n",
        "F1-score: The harmonic mean of precision and recall is the F1-score. A F1-score of 0.035 indicates that the model has a poor balance of precision and recall.\n"
      ],
      "metadata": {
        "id": "d9NgWf_6ffNW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Neural network model**:\n",
        "\n",
        "Building a neural network model that can learn and recognise patterns in the input data and forecast the related output is used in a Neural Networks-based technique to classifying the firm's rating into one of the rating categories and predicting if it is in an investment grade. This problem's input data is the firms' financial data, which includes numerous attributes such as liquidity ratios, profitability ratios, leverage ratios, and so on. The output variable is the firm's rating, which is a categorical variable with numerous possible values (for example, AAA, AA, A, BBB, and so on).\n",
        "\n",
        "Furthermore, the aim is to forecast if the firm is of investment grade or not, which is a categorical variable with two possible values (yes or no).\n",
        "\n",
        "Various neural network topologies, such as Feedforward Neural Networks, Convolutional Neural Networks, and Recurrent Neural Networks, can be used to develop a Neural Networks-based strategy. The model architecture is determined by the type of data and the complexity of the task. The model is trained on a labelled dataset that contains input features and associated output labels. The training procedure entails modifying the neural network's weights and biases to minimise the difference between the predicted and actual output. To evaluate the model's performance, it is validated using a different test dataset. Finally, the model can be used to forecast new, previously unknown data.\n"
      ],
      "metadata": {
        "id": "UFPvBCVO2t6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CODE**:\n",
        "\n",
        "The programme uses a **Neural Network-based** approach to categorise the firm's rating and predict if it is investment grade or not.\n",
        "\n",
        "The method begins by loading the credit score data file and utilising a dictionary to translate the rating categories to investment grade. The data is then divided into training and test sets, and one-hot encoding is applied to the target labels. To centre and normalise the data, the input features are scaled using **StandardScaler**.\n",
        "\n",
        "Following that, a **Sequential Neural Network** model with two hidden layers, dropout regularisation, and softmax activation is built. The model is built with **categorical_crossentropy** as a loss function, the **Adam** optimizer, and the **accuracy** metric in mind.\n",
        "\n",
        "The fit approach is used to train the **Sequential Neural Network model** with the training data, and the predict method is used to generate the predicted target labels for the test data. The accuracy_score function from the **scikit-learn** library is used to calculate the trained model's accuracy score.\n",
        "\n",
        "Finally, the code returns the anticipated and actual investment grade labels to the grading categories and outputs the model's accuracy score.\n",
        " "
      ],
      "metadata": {
        "id": "AfgC0yHn4xSv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5dQW_yP2X7PY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7e643fc-4c21-4aed-f6e9-58365b2599de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1700, 28)\n",
            "Epoch 1/50\n",
            "48/48 [==============================] - 1s 2ms/step - loss: 0.9371 - accuracy: 0.4935\n",
            "Epoch 2/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.7880 - accuracy: 0.6771\n",
            "Epoch 3/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.7224 - accuracy: 0.7144\n",
            "Epoch 4/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.6705 - accuracy: 0.7366\n",
            "Epoch 5/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.6371 - accuracy: 0.7471\n",
            "Epoch 6/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.6191 - accuracy: 0.7556\n",
            "Epoch 7/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.6019 - accuracy: 0.7634\n",
            "Epoch 8/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.6091 - accuracy: 0.7523\n",
            "Epoch 9/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.6096 - accuracy: 0.7497\n",
            "Epoch 10/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5763 - accuracy: 0.7562\n",
            "Epoch 11/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5883 - accuracy: 0.7549\n",
            "Epoch 12/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5839 - accuracy: 0.7484\n",
            "Epoch 13/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5879 - accuracy: 0.7523\n",
            "Epoch 14/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5793 - accuracy: 0.7529\n",
            "Epoch 15/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5513 - accuracy: 0.7588\n",
            "Epoch 16/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5564 - accuracy: 0.7634\n",
            "Epoch 17/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5499 - accuracy: 0.7536\n",
            "Epoch 18/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5750 - accuracy: 0.7588\n",
            "Epoch 19/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5459 - accuracy: 0.7582\n",
            "Epoch 20/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5437 - accuracy: 0.7641\n",
            "Epoch 21/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5546 - accuracy: 0.7556\n",
            "Epoch 22/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5499 - accuracy: 0.7595\n",
            "Epoch 23/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5392 - accuracy: 0.7634\n",
            "Epoch 24/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5317 - accuracy: 0.7614\n",
            "Epoch 25/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5533 - accuracy: 0.7542\n",
            "Epoch 26/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5420 - accuracy: 0.7562\n",
            "Epoch 27/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5335 - accuracy: 0.7582\n",
            "Epoch 28/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5370 - accuracy: 0.7608\n",
            "Epoch 29/50\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5488 - accuracy: 0.7575\n",
            "Epoch 30/50\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5317 - accuracy: 0.7569\n",
            "Epoch 31/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5297 - accuracy: 0.7582\n",
            "Epoch 32/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5256 - accuracy: 0.7641\n",
            "Epoch 33/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5246 - accuracy: 0.7575\n",
            "Epoch 34/50\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5208 - accuracy: 0.7654\n",
            "Epoch 35/50\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5260 - accuracy: 0.7641\n",
            "Epoch 36/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5282 - accuracy: 0.7634\n",
            "Epoch 37/50\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5097 - accuracy: 0.7621\n",
            "Epoch 38/50\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5188 - accuracy: 0.7673\n",
            "Epoch 39/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5199 - accuracy: 0.7582\n",
            "Epoch 40/50\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5095 - accuracy: 0.7673\n",
            "Epoch 41/50\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5223 - accuracy: 0.7647\n",
            "Epoch 42/50\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5196 - accuracy: 0.7699\n",
            "Epoch 43/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5138 - accuracy: 0.7641\n",
            "Epoch 44/50\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5269 - accuracy: 0.7647\n",
            "Epoch 45/50\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5170 - accuracy: 0.7621\n",
            "Epoch 46/50\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5052 - accuracy: 0.7725\n",
            "Epoch 47/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.5122 - accuracy: 0.7641\n",
            "Epoch 48/50\n",
            "48/48 [==============================] - 0s 2ms/step - loss: 0.4973 - accuracy: 0.7725\n",
            "Epoch 49/50\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5114 - accuracy: 0.7660\n",
            "Epoch 50/50\n",
            "48/48 [==============================] - 0s 3ms/step - loss: 0.5081 - accuracy: 0.7725\n",
            "6/6 [==============================] - 0s 2ms/step\n",
            "Accuracy of the trained sequential neural network model: 0.8\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "\n",
        "# loading the CreditScore data file\n",
        "creditScoreData = pd.read_csv(\"MLF_GP1_CreditScore.csv\")\n",
        "print(creditScoreData.shape)\n",
        "\n",
        "# Mapping ratings to investment grade\n",
        "ratingToInvestmentMap = {\n",
        "              'Baa1': 1, 'Baa2': 1, 'Baa3': 1, 'Ba1': 0, 'Ba2': 0, 'Ba3': 0,\n",
        "\t\t\t  'Aaa': 1, 'Aa1': 1, 'Aa2': 1, 'Aa3': 1, 'A1': 1, 'A2': 1, 'A3': 1,\n",
        "              'B1': 0, 'B2': 0, 'B3': 0, 'Caa1': 0}\n",
        "creditScoreData[\"InvGrd\"] = creditScoreData[\"Rating\"].map(ratingToInvestmentMap)\n",
        "\n",
        "# Dividing whole data into training and test sets\n",
        "inputFeatures_train, inputFeatures_test, target_train, target_test = train_test_split(creditScoreData.iloc[:, :-2], creditScoreData[\"InvGrd\"], test_size=0.1, random_state=50)\n",
        "\n",
        "# The to_categorical function produces a new array in which each element is transformed to a one-hot encoded vector\n",
        "target_train = to_categorical(target_train) \n",
        "target_test = to_categorical(target_test)\n",
        "\n",
        "# Scale the input features by centering and normalizing the data using the StandardScaler function\n",
        "stndrdSclr = StandardScaler()\n",
        "inputFeatures_train_scaled = stndrdSclr.fit_transform(inputFeatures_train)\n",
        "inputFeatures_test_scaled = stndrdSclr.transform(inputFeatures_test)\n",
        "\n",
        "# Create sequential neural network model with 2 hidden layers, dropout regularization, and softmax activation\n",
        "sequentialNNModel = Sequential()\n",
        "sequentialNNModel.add(Dense(32, input_dim=inputFeatures_train_scaled.shape[1], activation=\"relu\"))\n",
        "sequentialNNModel.add(Dropout(0.5))\n",
        "sequentialNNModel.add(Dense(16, activation=\"relu\"))\n",
        "sequentialNNModel.add(Dropout(0.5))\n",
        "sequentialNNModel.add(Dense(2, activation=\"softmax\"))\n",
        "sequentialNNModel.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "# train the sequential neural network model with scaled input features and target labels.\n",
        "sequentialNNModel.fit(inputFeatures_train_scaled, target_train, epochs=50, batch_size=32, verbose=1)\n",
        "\n",
        "# predicts target labels for the test data using the trained sequential neural network model\n",
        "predicted_target = sequentialNNModel.predict(inputFeatures_test_scaled)\n",
        "predicted_target_class = predicted_target.argmax(axis=-1)\n",
        "\n",
        "# map the predicted and actual investment grade labels back to the rating categories\n",
        "predicted_target_rating = [list(ratingToInvestmentMap.keys())[list(ratingToInvestmentMap.values()).index(i)] for i in predicted_target_class]\n",
        "target_test_rating = [list(ratingToInvestmentMap.keys())[list(ratingToInvestmentMap.values()).index(j)] for j in target_test.argmax(axis=-1)]\n",
        "\n",
        "# evaluate the model's performance\n",
        "SeqNNAccScore = accuracy_score(target_test.argmax(axis=-1), predicted_target_class)\n",
        "\n",
        "\n",
        "print(\"Accuracy of the trained sequential neural network model:\", SeqNNAccScore)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Accuracy of the trained sequential neural network model: 0.8058823529411765**\n",
        "\n",
        "The trained sequential neural network model accurately categorised 80.59% of the test data instances, resulting in an accuracy of 0.8058823529411765. In other words, the model accurately classified 80.59% of the test data cases. This is a measure of how well the model predicts whether a company's credit rating is investment grade or not.\n"
      ],
      "metadata": {
        "id": "0aISH26-P9_a"
      }
    }
  ]
}